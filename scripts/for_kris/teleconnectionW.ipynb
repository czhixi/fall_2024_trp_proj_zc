{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this under the pytorch (machine learning) conda environment.\n",
    "\n",
    "```powershell\n",
    "conda activate pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import rasterio\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import hypergeom\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "import cupy as cp\n",
    "import os\n",
    "from numba import jit, cuda\n",
    "import cupy_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_la_nina_sequences(df):\n",
    "    \"\"\"\n",
    "    Labels La Nina sequences where the maximum sequence number is greater than 5.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing La_Nina_Seq column\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original data with additional label column for significant sequences\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the label column\n",
    "    df_copy['sig_elnino'] = 0\n",
    "    df_copy['sig_lanina'] = 0\n",
    "    \n",
    "    # Function to find max in a sequence\n",
    "    def get_sequence_max(start_idx, seq_in):\n",
    "        sequence = []\n",
    "        idx = start_idx\n",
    "        while idx < len(df_copy) and df_copy[seq_in].iloc[idx] != 0:\n",
    "            sequence.append(df_copy[seq_in].iloc[idx])\n",
    "            idx += 1\n",
    "        return max(sequence) if sequence else 0\n",
    "\n",
    "    # Iterate through the dataframe\n",
    "    i = 0\n",
    "    seq = 'La_Nina_Seq'\n",
    "    while i < len(df_copy):\n",
    "        if df_copy[seq].iloc[i] == 1:  # Start of a sequence\n",
    "            max_in_sequence = get_sequence_max(i, seq)\n",
    "            if max_in_sequence > 5:\n",
    "                # Label all numbers in this sequence\n",
    "                j = i\n",
    "                while j < len(df_copy) and df_copy[seq].iloc[j] != 0:\n",
    "                    df_copy.loc[df_copy.index[j], 'sig_lanina'] = 1\n",
    "                    j += 1\n",
    "            # Skip to end of current sequence\n",
    "            while i < len(df_copy) and df_copy[seq].iloc[i] != 0:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "     # Iterate through the dataframe\n",
    "    i = 0\n",
    "    seq = 'El_Nino_Seq'\n",
    "    while i < len(df_copy):\n",
    "        if df_copy[seq].iloc[i] == 1:  # Start of a sequence\n",
    "            max_in_sequence = get_sequence_max(i, seq)\n",
    "            if max_in_sequence > 5:\n",
    "                # Label all numbers in this sequence\n",
    "                j = i\n",
    "                while j < len(df_copy) and df_copy[seq].iloc[j] != 0:\n",
    "                    df_copy.loc[df_copy.index[j], 'sig_elnino'] = 1\n",
    "                    j += 1\n",
    "            # Skip to end of current sequence\n",
    "            while i < len(df_copy) and df_copy[seq].iloc[i] != 0:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names manually\n",
    "column_monthly = [\"Year\", \"Month\", \"NINO1+2\", \"ANOM_NINO1+2\", \"NINO3\", \"ANOM_NINO3\",\n",
    "                \"NINO4\", \"ANOM_NINO4\", \"NINO3.4\", \"ANOM_NINO3.4\"]\n",
    "column_seasonal = ['SEAS', 'YR', 'TOTAL', 'ANOM']\n",
    "\n",
    "# Read the file\n",
    "# https://www.cpc.ncep.noaa.gov/data/indices/ersst5.nino.mth.91-20.ascii\n",
    "monthly = pd.read_csv(\"data/ersst5_nino_monthly.txt\", sep='\\s+', names=column_monthly, skiprows=1)[['Year', 'Month', 'NINO3.4', 'ANOM_NINO3.4']]\n",
    "seasonal = pd.read_csv(\"data/oni_seasonal.txt\", sep='\\s+', names=column_seasonal, skiprows=1)\n",
    "\n",
    "# Identify El Niño and La Niña periods\n",
    "seasonal['El_Nino'] = (seasonal['ANOM'] >= 0.5).astype(int)\n",
    "seasonal['La_Nina'] = (seasonal['ANOM'] <= -0.5).astype(int)\n",
    "\n",
    "# Compute running count of consecutive months where ONI exceeds 0.5 or is below -0.5\n",
    "seasonal['El_Nino_Seq'] = seasonal['El_Nino'] * seasonal['El_Nino'].groupby((seasonal['El_Nino'] != seasonal['El_Nino'].shift()).cumsum()).transform('cumsum')\n",
    "seasonal['La_Nina_Seq'] = seasonal['La_Nina'] * seasonal['La_Nina'].groupby((seasonal['La_Nina'] != seasonal['La_Nina'].shift()).cumsum()).transform('cumsum')\n",
    "\n",
    "enso_base = label_la_nina_sequences(seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_var = 'snowcover'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if climate_var == 'snowcover':\n",
    "    # Load NetCDF file\n",
    "    file_path = \"data/snow.nc\"\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    ds_values = ds['snowc'].values\n",
    "\n",
    "    lats = ds['latitude'].values\n",
    "    lons = ds['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
    "\n",
    "elif climate_var == 'snowalbedo':\n",
    "    # Load NetCDF file\n",
    "    file_path = \"data/snow.nc\"\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    ds_values = ds['asn'].values\n",
    "\n",
    "    lats = ds['latitude'].values\n",
    "    lons = ds['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
    "\n",
    "elif climate_var == 'net-solar':\n",
    "    file_path = \"data/solar1.nc\"\n",
    "    ds1 = xr.open_dataset(file_path)\n",
    "\n",
    "    file_path = \"data/solar2.nc\"\n",
    "    ds2 = xr.open_dataset(file_path)\n",
    "    # Concatenate along the time dimension\n",
    "\n",
    "    ds_values = np.concatenate([ds1['ssr'].values[:624], ds2['ssr'].values, ds1['ssr'].values[624:]], axis=0)\n",
    "\n",
    "    lats = ds1['latitude'].values\n",
    "    lons = ds1['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds1.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
    "\n",
    "elif climate_var == 'net-thermal':\n",
    "    file_path = \"data/solar1.nc\"\n",
    "    ds1 = xr.open_dataset(file_path)\n",
    "\n",
    "    file_path = \"data/solar2.nc\"\n",
    "    ds2 = xr.open_dataset(file_path)\n",
    "    # Concatenate along the time dimension\n",
    "\n",
    "    ds_values = np.concatenate([ds1['str'].values[:624], ds2['str'].values, ds1['str'].values[624:]], axis=0)\n",
    "\n",
    "    lats = ds1['latitude'].values\n",
    "    lons = ds1['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds1.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the world shapefile\n",
    "world = gpd.read_file('data/ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp')\n",
    "\n",
    "# Create a GeoDataFrame with all mesh grid points (assuming `coordinates` holds all lat/lon)\n",
    "points_gdf = gpd.GeoDataFrame(pd.DataFrame(coordinates, columns=['Longitude', 'Latitude']),\n",
    "                              geometry=gpd.points_from_xy(coordinates[:, 0], coordinates[:, 1]), crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join to select only land-based points (union of all country polygons)\n",
    "land_points = gpd.sjoin(points_gdf, world, op='within')\n",
    "\n",
    "# Extract indices for all valid land points\n",
    "all_points_idx_climate = []  # This replaces `points_idx_climate`\n",
    "for lon, lat in land_points[['Longitude', 'Latitude']].values:\n",
    "    lat_idx = abs(ds['latitude'] - lat).argmin().item()\n",
    "    lon_idx = abs(ds['longitude'] - lon).argmin().item()\n",
    "    all_points_idx_climate.append([lat_idx, lon_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data to a pickle file\n",
    "# with open(\"outcome/lon_lat_idx_era5_w.pkl\", \"wb\") as file:\n",
    "    #pickle.dump(all_points_idx_climate, file)\n",
    "\n",
    "with open(\"outcome/lon_lat_idx_era5_w.pkl\", \"rb\") as file:\n",
    "    all_points_idx_climate = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lats = range(1801)\n",
    "# lons = range(3600)\n",
    "# # Create meshgrid of lat/lon pairs\n",
    "# lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "# # Flatten the grids and create coordinate pairs\n",
    "# coordinates_idx = list(zip(lat_grid.ravel(), lon_grid.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_order = [\"DJF\", \"JFM\", \"FMA\", \"MAM\", \"AMJ\", \"MJJ\", \"JJA\", \"JAS\", \"ASO\", \"SON\", \"OND\", \"NDJ\"]\n",
    "states = ['El Nino', 'La Nina', 'Normal']\n",
    "tercile_order = ['BN', 'NN', 'AN']\n",
    "states_lower = ['elnino', 'lanina', 'normal']\n",
    "\n",
    "seasons_all = enso_base['SEAS'].values\n",
    "sig_elnino = enso_base['sig_elnino'].values\n",
    "sig_lanina = enso_base['sig_lanina'].values\n",
    "enso_masks = [\n",
    "        sig_elnino == 1,  # El Nino\n",
    "        sig_lanina == 1,  # La Nina\n",
    "        (sig_elnino == 0) & (sig_lanina == 0)  # Neutral\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/2141055 [00:03<101:03:08,  5.89it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'numpy.nanquantile' on types that implement __array_function__: [<class 'cupy.ndarray'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m values_mon \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mreshape(values, (\u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m12\u001b[39m))\u001b[38;5;241m.\u001b[39mT  \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Compute terciles (33rd and 67th percentiles) for each month\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m tercile \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_mon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.33\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.67\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Convert values into tercile categories (-1: below, 0: normal, 1: above)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m tercile_binary \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     27\u001b[0m     [values_mon\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tercile[\u001b[38;5;241m0\u001b[39m], values_mon\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m>\u001b[39m tercile[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     28\u001b[0m     [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     29\u001b[0m     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m )\u001b[38;5;241m.\u001b[39mflatten()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Flatten and remove last value to match length\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: no implementation found for 'numpy.nanquantile' on types that implement __array_function__: [<class 'cupy.ndarray'>]"
     ]
    }
   ],
   "source": [
    "# Initialize global dictionaries for storing results\n",
    "freq_all_world = {}  \n",
    "significance_all_world = {}\n",
    "\n",
    "# Initialize lists to store global data\n",
    "freq_all = []\n",
    "significance_all = []\n",
    "\n",
    "# Process all grid points worldwide\n",
    "for lat, lon in tqdm(all_points_idx_climate):  # Assuming `all_points_idx_climate` contains global points\n",
    "    values = cp.array(ds_values[:, lat, lon])  # Extract climate data for this grid point\n",
    "    \n",
    "    # If all values are NaN, append NaN-filled arrays and continue\n",
    "    if np.all(np.isnan(values)):  \n",
    "        freq_all.append(np.full((3, len(season_order), 3), np.nan))\n",
    "        significance_all.append(np.full((3, 2), np.nan))\n",
    "        continue  \n",
    "\n",
    "    # Reshape data to 75 years × 12 months\n",
    "    values_mon = cp.reshape(values, (75, 12)).T  \n",
    "\n",
    "    # Compute terciles (33rd and 67th percentiles) for each month\n",
    "    tercile = np.nanquantile(values_mon, [0.33, 0.67], axis=1)  \n",
    "\n",
    "    # Convert values into tercile categories (-1: below, 0: normal, 1: above)\n",
    "    tercile_binary = cp.select(\n",
    "        [values_mon.T <= tercile[0], values_mon.T > tercile[1]],\n",
    "        [-1, 1],\n",
    "        default=0\n",
    "    ).flatten()[:-1]  # Flatten and remove last value to match length\n",
    "\n",
    "    # Initialize frequency array (3 ENSO states × seasons × 3 terciles)\n",
    "    frequencies = np.zeros((3, len(season_order), 3))  \n",
    "\n",
    "    # Compute tercile frequency per ENSO state and season\n",
    "    for state_idx, state_mask in enumerate(enso_masks):  \n",
    "        for seas_idx, season in enumerate(season_order):  \n",
    "            season_mask = seasons_all == season  \n",
    "            combined_mask = state_mask & season_mask  # Mask for ENSO & season\n",
    "            \n",
    "            if np.any(combined_mask):  \n",
    "                for tercile in [-1, 0, 1]:  \n",
    "                    tercile_count = np.sum((tercile_binary == tercile) & combined_mask)  \n",
    "                    frequencies[state_idx, seas_idx, tercile + 1] = tercile_count / np.sum(combined_mask)\n",
    "\n",
    "    # Compute total number of months\n",
    "    total_months = len(tercile_binary)  \n",
    "\n",
    "    # Create masks for months above and below normal\n",
    "    above_mask = tercile_binary == 1  \n",
    "    below_mask = tercile_binary == -1  \n",
    "\n",
    "    # Count total months in each category\n",
    "    total_above = np.sum(above_mask)  \n",
    "    total_below = np.sum(below_mask)  \n",
    "\n",
    "    # Initialize arrays to store observed counts and p-values\n",
    "    enso_months = np.zeros(3)  \n",
    "    observed_above = np.zeros(3)  \n",
    "    observed_below = np.zeros(3)  \n",
    "    p_values = np.zeros((3, 2))  # [above, below] p-values for each ENSO state\n",
    "\n",
    "    # Compute statistical significance using hypergeometric test\n",
    "    for i, mask in enumerate(enso_masks):  \n",
    "        enso_months[i] = np.sum(mask)  # Total months in this ENSO state\n",
    "        observed_above[i] = np.sum(mask & above_mask)  # Above normal counts\n",
    "        observed_below[i] = np.sum(mask & below_mask)  # Below normal counts\n",
    "\n",
    "        # Compute p-value for above-normal months\n",
    "        p_values[i, 0] = hypergeom.sf(\n",
    "            observed_above[i] - 1,  # k-1 (observed successes minus 1)\n",
    "            total_months,  # N (total population)\n",
    "            total_above,  # K (total successes in population)\n",
    "            enso_months[i]  # n (sample size)\n",
    "        )\n",
    "\n",
    "        # Compute p-value for below-normal months\n",
    "        p_values[i, 1] = hypergeom.sf(\n",
    "            observed_below[i] - 1,  # k-1 (observed successes minus 1)\n",
    "            total_months,  # N (total population)\n",
    "            total_below,  # K (total successes in population)\n",
    "            enso_months[i]  # n (sample size)\n",
    "        )\n",
    "\n",
    "    # Store frequency and significance results for this grid point\n",
    "    freq_all.append(frequencies)  \n",
    "    significance_all.append(p_values)  \n",
    "\n",
    "# Store global results\n",
    "freq_all_world[\"global\"] = freq_all  \n",
    "significance_all_world[\"global\"] = significance_all  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a pickle file\n",
    "with open(f\"outcome/frequency_w_{climate_var}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(freq_all_world, file)\n",
    "\n",
    "with open(f\"outcome/significance_w_{climate_var}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(significance_all_world, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the frequency data\n",
    "with open(f\"outcome/frequency_w_{climate_var}.pkl\", \"rb\") as file:\n",
    "    freq_all_world = pickle.load(file)\n",
    "\n",
    "# Load the significance data\n",
    "with open(f\"outcome/significance_w_{climate_var}.pkl\", \"rb\") as file:\n",
    "    significance_all_world = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_colormaps():\n",
    "    \"\"\"\n",
    "    Create two custom colormaps for Below Normal and Above Normal based on the image\n",
    "    \"\"\"\n",
    "    # Below Normal colors (yellow to brown)\n",
    "    below_colors = ['#f9fa04', '#e7b834', '#ce8033', '#a9451d', '#783100']\n",
    "    \n",
    "    # Above Normal colors (light green to blue)\n",
    "    above_colors = ['#d1f8cb', '#adf79f', '#75ba6f', '#4394cc', '#0c3af3']\n",
    "    \n",
    "    below_cmap = ListedColormap(below_colors)\n",
    "    above_cmap = ListedColormap(above_colors)\n",
    "    \n",
    "    return below_cmap, above_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probability_maps(data_all, index_all, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    base_map = np.full((1801, 3600), np.nan)\n",
    "    below_cmap, above_cmap = create_custom_colormaps()\n",
    "    \n",
    "    projection = ccrs.PlateCarree(central_longitude=180)\n",
    "    data_transform = ccrs.PlateCarree()\n",
    "    \n",
    "    for season in tqdm(range(12)):\n",
    "        for state in range(3):\n",
    "            category_map = base_map.copy()\n",
    "            probability_map = base_map.copy()\n",
    "            data = np.array(data_all['global'])\n",
    "\n",
    "            for coord_idx, (lat_idx, lon_idx) in enumerate(index_all):\n",
    "                probs = data[coord_idx, state, season, :]\n",
    "                max_prob = np.max(probs)\n",
    "                max_state = np.argmax(probs)\n",
    "                \n",
    "                category_map[lat_idx, lon_idx] = max_state\n",
    "                probability_map[lat_idx, lon_idx] = max_prob * 100\n",
    "            \n",
    "            fig = plt.figure(figsize=(15, 8))\n",
    "            ax = plt.axes(projection=projection)\n",
    "            ax.set_title(f'State {states[state]}, Season {season_order[season]}', fontsize=14, pad=15, fontweight='bold')\n",
    "            ax.coastlines(linewidth=0.5)\n",
    "            ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "            \n",
    "            below_mask = category_map == 0\n",
    "            above_mask = category_map == 2\n",
    "            \n",
    "            below_probs = np.ma.masked_where(~below_mask, probability_map)\n",
    "            ax.imshow(below_probs, transform=data_transform, extent=[0, 360, -90, 90], cmap=below_cmap, vmin=40, vmax=70)\n",
    "            \n",
    "            above_probs = np.ma.masked_where(~above_mask, probability_map)\n",
    "            ax.imshow(above_probs, transform=data_transform, extent=[0, 360, -90, 90], cmap=above_cmap, vmin=40, vmax=70)\n",
    "            \n",
    "            norm = plt.Normalize(40, 70)\n",
    "            sm_below = plt.cm.ScalarMappable(cmap=below_cmap, norm=norm)\n",
    "            sm_above = plt.cm.ScalarMappable(cmap=above_cmap, norm=norm)\n",
    "            \n",
    "            cbar_below_ax = fig.add_axes([0.125, 0.05, 0.35, 0.02])\n",
    "            cbar_below = fig.colorbar(sm_below, cax=cbar_below_ax, orientation='horizontal')\n",
    "            cbar_below.set_label('Below Normal')\n",
    "            \n",
    "            cbar_above_ax = fig.add_axes([0.525, 0.05, 0.35, 0.02])\n",
    "            cbar_above = fig.colorbar(sm_above, cax=cbar_above_ax, orientation='horizontal')\n",
    "            cbar_above.set_label('Above Normal')\n",
    "            \n",
    "            fig.text(0.5, 0.15, 'Probability (%) of Most Likely Category', ha='center', va='center', fontsize=10)\n",
    "            plt.subplots_adjust(bottom=0.2)\n",
    "            \n",
    "            filename = f'prob_{states_lower[state]}_season_{season_order[season]}.png'\n",
    "            plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dual_p_value_maps(data_all, index_all, output_dir=f'outcome/map/{climate_var}'):\n",
    "    \"\"\"\n",
    "    Create side-by-side maps showing p-values for above (blue) and below (red)\n",
    "    \"\"\"\n",
    "    # Create colormaps\n",
    "    # Blue scheme for above\n",
    "    colors_above = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'][::-1]\n",
    "    cmap_above = LinearSegmentedColormap.from_list('custom_blues', colors_above)\n",
    "    \n",
    "    # Red scheme for below\n",
    "    colors_below = ['#fff5f0', '#fee0d2', '#fcbba1', '#fc9272', '#fb6a4a', '#ef3b2c', '#cb181d', '#99000d'][::-1]\n",
    "    cmap_below = LinearSegmentedColormap.from_list('custom_reds', colors_below)\n",
    "    \n",
    "    \n",
    "    for state in tqdm(range(3)):\n",
    "        # Create a new map for this season and state\n",
    "        base_map_above = np.full((1801, 3600), np.nan)\n",
    "        base_map_below = np.full((1801, 3600), np.nan)\n",
    "        \n",
    "        data = np.array(data_all['global'])\n",
    "        for coord_idx, (lat_idx, lon_idx) in enumerate(index_all):\n",
    "            # Get probabilities for all states in this season\n",
    "            probs_ab = data[coord_idx, state , 0]\n",
    "            probs_bl = data[coord_idx, state , 1]\n",
    "            \n",
    "            base_map_above[lat_idx, lon_idx] = probs_ab\n",
    "            base_map_below[lat_idx, lon_idx] = probs_bl\n",
    "\n",
    "        # Create the plot\n",
    "        fig = plt.figure(figsize=(20, 8))\n",
    "        \n",
    "        # Create projection\n",
    "        projection = ccrs.PlateCarree(central_longitude=180)\n",
    "        data_transform = ccrs.PlateCarree()\n",
    "        \n",
    "        # Above Normal p-values (left plot)\n",
    "        ax1 = plt.subplot(121, projection=projection)\n",
    "        ax1.coastlines(linewidth=0.5)\n",
    "        ax1.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "        \n",
    "        img1 = ax1.imshow(base_map_above,\n",
    "                        transform=data_transform,\n",
    "                        extent=[0, 360, -90, 90],\n",
    "                        cmap=cmap_above,\n",
    "                        vmin=0,\n",
    "                        vmax=1)\n",
    "        \n",
    "        # Add colorbar for above\n",
    "        cbar1 = plt.colorbar(img1, orientation='horizontal', pad=0.1)\n",
    "        cbar1.set_label('P-value Above Normal')\n",
    "        \n",
    "        # Add gridlines\n",
    "        gl1 = ax1.gridlines(draw_labels=True, linewidth=0.2, color='gray', alpha=0.5)\n",
    "        gl1.top_labels = False\n",
    "        gl1.right_labels = False\n",
    "        \n",
    "        # Title for left plot\n",
    "        ax1.set_title('Above Normal P-values', fontsize=12, pad=10)\n",
    "        \n",
    "        # Below Normal p-values (right plot)\n",
    "        ax2 = plt.subplot(122, projection=projection)\n",
    "        ax2.coastlines(linewidth=0.5)\n",
    "        ax2.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "        \n",
    "        img2 = ax2.imshow(base_map_below,\n",
    "                        transform=data_transform,\n",
    "                        extent=[0, 360, -90, 90],\n",
    "                        cmap=cmap_below,\n",
    "                        vmin=0,\n",
    "                        vmax=1)\n",
    "        \n",
    "        # Add colorbar for below\n",
    "        cbar2 = plt.colorbar(img2, orientation='horizontal', pad=0.1)\n",
    "        cbar2.set_label('P-value Below Normal')\n",
    "        \n",
    "        # Add gridlines\n",
    "        gl2 = ax2.gridlines(draw_labels=True, linewidth=0.2, color='gray', alpha=0.5)\n",
    "        gl2.top_labels = False\n",
    "        gl2.right_labels = False\n",
    "        \n",
    "        # Title for right plot\n",
    "        ax2.set_title('Below Normal P-values', fontsize=12, pad=10)\n",
    "        \n",
    "        filename = f'pvalue_maps_{states_lower[state]}.png'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(filepath, \n",
    "                   dpi=300,              # Resolution\n",
    "                   bbox_inches='tight',   # Trim extra white space\n",
    "                   facecolor='white',     # White background\n",
    "                   format='png')          # File format\n",
    "        \n",
    "        # Show the plot (optional)\n",
    "        # plt.show()\n",
    "        \n",
    "        # Close the figure to free memory\n",
    "        plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]c:\\Users\\Kris\\AppData\\Local\\anaconda3\\envs\\pytorch\\lib\\site-packages\\cartopy\\io\\__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/110m_physical/ne_110m_coastline.zip\n",
      "  warnings.warn(f'Downloading: {url}', DownloadWarning)\n",
      "c:\\Users\\Kris\\AppData\\Local\\anaconda3\\envs\\pytorch\\lib\\site-packages\\cartopy\\io\\__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_boundary_lines_land.zip\n",
      "  warnings.warn(f'Downloading: {url}', DownloadWarning)\n",
      "100%|██████████| 12/12 [05:49<00:00, 29.11s/it]\n"
     ]
    }
   ],
   "source": [
    "create_probability_maps(freq_all_world, all_points_idx_climate, output_dir=f'outcome/map/{climate_var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.18s/it]\n"
     ]
    }
   ],
   "source": [
    "create_dual_p_value_maps(significance_all_world, all_points_idx_climate, output_dir=f'outcome/map/{climate_var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
